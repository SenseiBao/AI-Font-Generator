{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ckHwxmP0MaA",
        "outputId": "ad60882e-4086-401c-b1f0-ef4c3bd6efc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/FontGenerator\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/FontGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekANRnHb_a6g",
        "outputId": "a2847779-7da1-4e9a-b39e-d9aded99ec16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Images', 'character_fonts (with handwritten data).npz', 'character_font.npz']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "with zipfile.ZipFile('fonts.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/unzipped_fonts')\n",
        "\n",
        "# Check the extracted contents\n",
        "os.listdir('/content/unzipped_fonts')\n",
        "\n",
        "\n",
        "# Load a specific font data from an unzipped .npy file\n",
        "#font_data = np.load('unzipped_fonts/font_A.npy')  # Adjust the path and filename\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCaW3jiLFiIu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elJ7YwMPFkIG",
        "outputId": "65d064fd-5b82-400d-e280-29172161380a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded batch 1 with size: torch.Size([192, 1, 64, 64])\n",
            "Loaded batch 2 with size: torch.Size([192, 1, 64, 64])\n",
            "Loaded batch 3 with size: torch.Size([192, 1, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# path to dataset\n",
        "dataset_path = '/content/unzipped_fonts/Images/'\n",
        "\n",
        "# image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.Resize((64, 64)),   # Resize\n",
        "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for GAN\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Create the DataLoader\n",
        "batch_size = 192  # Set batch size for training\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# Check the first few batches\n",
        "for i, (images, _) in enumerate(dataloader):\n",
        "    print(f\"Loaded batch {i + 1} with size: {images.size()}\")\n",
        "    if i >= 2:  # Limit to a few batches\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Z7gw05pe1W",
        "outputId": "b965ffe9-fa5b-458d-f98a-d9ddcfead35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in dataset: 389765\n",
            "Image 0 shape: torch.Size([1, 64, 64]), label: 0\n",
            "Image 1 shape: torch.Size([1, 64, 64]), label: 0\n",
            "Image 2 shape: torch.Size([1, 64, 64]), label: 0\n",
            "Image 3 shape: torch.Size([1, 64, 64]), label: 0\n",
            "Image 4 shape: torch.Size([1, 64, 64]), label: 0\n"
          ]
        }
      ],
      "source": [
        "# Check the length of the dataset\n",
        "print(f\"Number of images in dataset: {len(dataset)}\")\n",
        "\n",
        "# Attempt to load a few images directly\n",
        "for i in range(5):  # Try to load the first 5 images\n",
        "    image, label = dataset[i]\n",
        "    print(f\"Image {i} shape: {image.shape}, label: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQqWSb4uFpVI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lyiz4xAlFtgD"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4096 + 26, 2048),  # Input latent vector + 26 for one-hot letter encoding\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 64 * 64),  # Output flattened image of size 4096\n",
        "            nn.Tanh()  # Normalize to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, label):\n",
        "        # Concatenate the noise vector and one-hot encoded label\n",
        "        x = torch.cat((noise, label), dim=1)\n",
        "        x = self.model(x)\n",
        "        x = x.view(-1, 1, 64, 64)  # Reshape to (batch_size, 1 channel, 64x64)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU0rj3KkFubU"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(64 * 64 + 26, 1024),  # Flattened image + one-hot letter encoding\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),  # Real/fake output\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, label):\n",
        "        # Flatten the image and concatenate with the label\n",
        "        img = img.view(img.size(0), -1)\n",
        "        x = torch.cat((img, label), dim=1)\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRiZuSe-itfC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    \"\"\"\n",
        "    Convert labels to one-hot encoded vectors.\n",
        "    Args:\n",
        "        labels: A tensor of labels (integers).\n",
        "        num_classes: The number of classes (e.g., 26 for the alphabet).\n",
        "    Returns:\n",
        "        One-hot encoded labels of shape (batch_size, num_classes).\n",
        "    \"\"\"\n",
        "    return torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNv_l3J9D-lq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Function to save the model, optimizer, and epoch\n",
        "def save_checkpoint(generator, discriminator, optimizer_G, optimizer_D, epoch, filename='checkpoint.pth'):\n",
        "    checkpoint = {\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "    print(f\"Checkpoint saved at epoch {epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEqff8_BD_23"
      },
      "outputs": [],
      "source": [
        "# Function to load the model, optimizer, and epoch\n",
        "def load_checkpoint(generator, discriminator, optimizer_G, optimizer_D, filename='checkpoint.pth'):\n",
        "    with open(filename, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print(f\"Checkpoint loaded, resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9dLbFAECFxm9",
        "outputId": "5f34911e-0f73-4d2e-c73c-f6fe304fd835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded, resuming from epoch 57\n",
            "Starting epoch 58/100\n",
            "Checkpoint saved at epoch 57\n",
            "Epoch [58/100]  Loss D: 0.29371801018714905, Loss G: 16.26841163635254\n",
            "Starting epoch 59/100\n",
            "Checkpoint saved at epoch 58\n",
            "Epoch [59/100]  Loss D: 0.391812264919281, Loss G: 4.614320278167725\n",
            "Starting epoch 60/100\n",
            "Checkpoint saved at epoch 59\n",
            "Epoch [60/100]  Loss D: 0.13217365741729736, Loss G: 7.454075813293457\n",
            "Starting epoch 61/100\n",
            "Checkpoint saved at epoch 60\n",
            "Epoch [61/100]  Loss D: 0.19985803961753845, Loss G: 4.3376383781433105\n",
            "Starting epoch 62/100\n",
            "Checkpoint saved at epoch 61\n",
            "Epoch [62/100]  Loss D: 0.2496483027935028, Loss G: 13.598889350891113\n",
            "Starting epoch 63/100\n",
            "Checkpoint saved at epoch 62\n",
            "Epoch [63/100]  Loss D: 0.2742679715156555, Loss G: 10.855316162109375\n",
            "Starting epoch 64/100\n",
            "Checkpoint saved at epoch 63\n",
            "Epoch [64/100]  Loss D: 0.020071817561984062, Loss G: 5.376232147216797\n",
            "Starting epoch 65/100\n",
            "Checkpoint saved at epoch 64\n",
            "Epoch [65/100]  Loss D: 0.09332498162984848, Loss G: 7.308335304260254\n",
            "Starting epoch 66/100\n",
            "Checkpoint saved at epoch 65\n",
            "Epoch [66/100]  Loss D: 0.9825073480606079, Loss G: 4.189876079559326\n",
            "Starting epoch 67/100\n",
            "Checkpoint saved at epoch 66\n",
            "Epoch [67/100]  Loss D: 0.39063671231269836, Loss G: 4.039605617523193\n",
            "Starting epoch 68/100\n",
            "Checkpoint saved at epoch 67\n",
            "Epoch [68/100]  Loss D: 0.590146541595459, Loss G: 9.079080581665039\n",
            "Starting epoch 69/100\n",
            "Checkpoint saved at epoch 68\n",
            "Epoch [69/100]  Loss D: 0.060026898980140686, Loss G: 5.573700428009033\n",
            "Starting epoch 70/100\n",
            "Checkpoint saved at epoch 69\n",
            "Epoch [70/100]  Loss D: 0.08187412470579147, Loss G: 6.425452709197998\n",
            "Starting epoch 71/100\n",
            "Checkpoint saved at epoch 70\n",
            "Epoch [71/100]  Loss D: 0.16808554530143738, Loss G: 8.764640808105469\n",
            "Starting epoch 72/100\n",
            "Checkpoint saved at epoch 71\n",
            "Epoch [72/100]  Loss D: 0.42292216420173645, Loss G: 12.33459758758545\n",
            "Starting epoch 73/100\n",
            "Checkpoint saved at epoch 72\n",
            "Epoch [73/100]  Loss D: 0.505036473274231, Loss G: 19.133569717407227\n",
            "Starting epoch 74/100\n",
            "Checkpoint saved at epoch 73\n",
            "Epoch [74/100]  Loss D: 0.0754295215010643, Loss G: 5.226101398468018\n",
            "Starting epoch 75/100\n",
            "Checkpoint saved at epoch 74\n",
            "Epoch [75/100]  Loss D: 0.12136951833963394, Loss G: 5.4880290031433105\n",
            "Starting epoch 76/100\n",
            "Checkpoint saved at epoch 75\n",
            "Epoch [76/100]  Loss D: 0.1953458935022354, Loss G: 7.605123043060303\n",
            "Starting epoch 77/100\n",
            "Checkpoint saved at epoch 76\n",
            "Epoch [77/100]  Loss D: 1.1966147422790527, Loss G: 2.644015073776245\n",
            "Starting epoch 78/100\n",
            "Checkpoint saved at epoch 77\n",
            "Epoch [78/100]  Loss D: 0.025126583874225616, Loss G: 6.773619174957275\n",
            "Starting epoch 79/100\n",
            "Checkpoint saved at epoch 78\n",
            "Epoch [79/100]  Loss D: 0.3794292211532593, Loss G: 2.520200252532959\n",
            "Starting epoch 80/100\n",
            "Checkpoint saved at epoch 79\n",
            "Epoch [80/100]  Loss D: 0.7261970639228821, Loss G: 1.5256508588790894\n",
            "Starting epoch 81/100\n",
            "Checkpoint saved at epoch 80\n",
            "Epoch [81/100]  Loss D: 0.1470586657524109, Loss G: 8.888897895812988\n",
            "Starting epoch 82/100\n",
            "Checkpoint saved at epoch 81\n",
            "Epoch [82/100]  Loss D: 0.6292737126350403, Loss G: 16.58138084411621\n",
            "Starting epoch 83/100\n",
            "Checkpoint saved at epoch 82\n",
            "Epoch [83/100]  Loss D: 0.2756957709789276, Loss G: 23.53511619567871\n",
            "Starting epoch 84/100\n",
            "Checkpoint saved at epoch 83\n",
            "Epoch [84/100]  Loss D: 0.02764047309756279, Loss G: 6.778506755828857\n",
            "Starting epoch 85/100\n",
            "Checkpoint saved at epoch 84\n",
            "Epoch [85/100]  Loss D: 0.024170588701963425, Loss G: 5.914849758148193\n",
            "Starting epoch 86/100\n",
            "Checkpoint saved at epoch 85\n",
            "Epoch [86/100]  Loss D: 0.0238048005849123, Loss G: 6.168610572814941\n",
            "Starting epoch 87/100\n",
            "Checkpoint saved at epoch 86\n",
            "Epoch [87/100]  Loss D: 0.03507395088672638, Loss G: 5.382857799530029\n",
            "Starting epoch 88/100\n",
            "Checkpoint saved at epoch 87\n",
            "Epoch [88/100]  Loss D: 0.0670735239982605, Loss G: 7.468364715576172\n",
            "Starting epoch 89/100\n",
            "Checkpoint saved at epoch 88\n",
            "Epoch [89/100]  Loss D: 0.4664134085178375, Loss G: 29.69782066345215\n",
            "Starting epoch 90/100\n",
            "Checkpoint saved at epoch 89\n",
            "Epoch [90/100]  Loss D: 0.625868022441864, Loss G: 1.2476781606674194\n",
            "Starting epoch 91/100\n",
            "Checkpoint saved at epoch 90\n",
            "Epoch [91/100]  Loss D: 0.1771441549062729, Loss G: 9.238604545593262\n",
            "Starting epoch 92/100\n",
            "Checkpoint saved at epoch 91\n",
            "Epoch [92/100]  Loss D: 0.023757891729474068, Loss G: 4.990209102630615\n",
            "Starting epoch 93/100\n",
            "Checkpoint saved at epoch 92\n",
            "Epoch [93/100]  Loss D: 0.251955509185791, Loss G: 7.961175441741943\n",
            "Starting epoch 94/100\n",
            "Checkpoint saved at epoch 93\n",
            "Epoch [94/100]  Loss D: 0.07679123431444168, Loss G: 10.064136505126953\n",
            "Starting epoch 95/100\n",
            "Checkpoint saved at epoch 94\n",
            "Epoch [95/100]  Loss D: 0.05149850994348526, Loss G: 10.4128999710083\n",
            "Starting epoch 96/100\n",
            "Checkpoint saved at epoch 95\n",
            "Epoch [96/100]  Loss D: 0.01856228895485401, Loss G: 5.974732398986816\n",
            "Starting epoch 97/100\n",
            "Checkpoint saved at epoch 96\n",
            "Epoch [97/100]  Loss D: 0.09708922356367111, Loss G: 16.995651245117188\n",
            "Starting epoch 98/100\n",
            "Checkpoint saved at epoch 97\n",
            "Epoch [98/100]  Loss D: 0.03800208494067192, Loss G: 6.631788730621338\n",
            "Starting epoch 99/100\n",
            "Checkpoint saved at epoch 98\n",
            "Epoch [99/100]  Loss D: 0.054078590124845505, Loss G: 9.481661796569824\n",
            "Starting epoch 100/100\n",
            "Checkpoint saved at epoch 99\n",
            "Epoch [100/100]  Loss D: 0.06810140609741211, Loss G: 4.136707305908203\n"
          ]
        }
      ],
      "source": [
        "# Instantiate generator and discriminator\n",
        "generator = Generator().cuda()       # If you're using GPU (add .cuda())\n",
        "discriminator = Discriminator().cuda()  # Same for the discriminator\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100  # Set number of epochs for training\n",
        "\n",
        "start_epoch = 0  # Default start epoch\n",
        "\n",
        "# Load checkpoint if resuming\n",
        "try:\n",
        "    start_epoch = load_checkpoint(generator, discriminator, optimizer_G, optimizer_D, 'checkpoint.pth')\n",
        "except FileNotFoundError:\n",
        "    print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for i, (real_images, labels) in enumerate(dataloader):\n",
        "        real_images = real_images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # One-hot encode the labels (if necessary)\n",
        "        labels_one_hot = one_hot_encode(labels, num_classes=26).cuda()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Train on real images\n",
        "        real_labels = torch.ones(batch_size, 1).cuda()\n",
        "        real_loss = criterion(discriminator(real_images, labels_one_hot), real_labels)\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = torch.randn(batch_size, 4096).cuda()\n",
        "        fake_images = generator(noise, labels_one_hot)\n",
        "\n",
        "        # Train on fake images\n",
        "        fake_labels = torch.zeros(batch_size, 1).cuda()\n",
        "        fake_loss = criterion(discriminator(fake_images.detach(), labels_one_hot), fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generator loss (attempting to fool the discriminator)\n",
        "        g_loss = criterion(discriminator(fake_images, labels_one_hot), real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    save_checkpoint(generator, discriminator, optimizer_G, optimizer_D, epoch, filename='checkpoint.pth')\n",
        "\n",
        "    # Print loss after every epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]  Loss D: {d_loss.item()}, Loss G: {g_loss.item()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4JaueVzF027"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_specific_letter(letter):\n",
        "    # Map the letter to an integer label (A=0, B=1, ..., Z=25)\n",
        "    letter_idx = ord(letter.upper()) - ord('A')\n",
        "\n",
        "    # Create a one-hot encoding for the letter\n",
        "    label = one_hot_encode(torch.tensor([letter_idx]), num_classes=26).cuda()\n",
        "\n",
        "    # Generate random noise\n",
        "    noise = torch.randn(1, 4096).cuda()\n",
        "\n",
        "    # Generate the image\n",
        "    generated_image = generator(noise, label)\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.imshow(generated_image.squeeze().cpu().detach().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Generate the letter 'A'\n",
        "generate_specific_letter('o')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}